# -*- coding: utf-8 -*-
"""
Created on Mon Apr 28 22:43:23 2025

@author: Lucius
"""

import pandas as pd
import os
from sklearn.preprocessing import OneHotEncoder, MinMaxScaler
import tkinter as tk
from tkinter import filedialog
import sys
from factor_analyzer import FactorAnalyzer
from semopy import Model
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
import matplotlib as mpl
from scipy import stats
from scipy.stats import norm
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from mpl_toolkits.mplot3d import Axes3D
from semopy import calc_stats
from graphviz import Digraph
import warnings

warnings.filterwarnings('ignore')
    
    
import stata_setup
stata_setup.config(r"C:\Program Files\Stata18","mp") #åªå¡«å†™è·¯å¾„è€Œéexe

from pystata import stata


root = tk.Tk()
root.withdraw()


print("è¯·é€‰æ‹©ç”¨æˆ·æ•°æ®æ–‡ä»¶ï¼ˆä»…æ”¯æŒExcelï¼‰...")
file_path = filedialog.askopenfilename(filetypes=[("Excel files", "*.xlsx;*.xls")])
if not file_path:
    print("æœªé€‰æ‹©æ–‡ä»¶ï¼Œç¨‹åºé€€å‡ºã€‚")
    sys.exit()
    
print("è¯·é€‰æ‹©è¾“å‡ºæ–‡ä»¶å¤¹...")
output_path = filedialog.askdirectory()
if not output_path:
    print("æœªé€‰æ‹©è¾“å‡ºæ–‡ä»¶å¤¹ï¼Œç¨‹åºé€€å‡ºã€‚")
    sys.exit()

r'''
file_path = r"C:\Users\Lucius\Desktop\å¸‚è°ƒèµ›;Zä¸–ä»£å¯¹æƒ…æ„Ÿé™ªä¼´AIçš„æ¶ˆè´¹æ€åº¦ä¸å¸‚åœºæ½œåŠ›ç ”ç©¶\é—®å·æ•°æ®å¤„ç†\é—®å·æ•°æ® åŸå§‹æ•´åˆ.xlsx"
output_path = r"C:\Users\Lucius\Desktop\output"
'''

os.makedirs(output_path, exist_ok = True)
try:
    df = pd.read_excel(file_path)
except FileNotFoundError:
    print("é”™è¯¯ï¼šæœªæ‰¾åˆ°æŒ‡å®šçš„æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„ã€‚")
    sys.exit()
else:
    ''' æ•°æ®æ¸…æ´— '''
    
    #åˆ å»åˆ—
    columns_to_drop = ['åºå·', 'æäº¤ç­”å·æ—¶é—´', 'æ‰€ç”¨æ—¶é—´', 'æ¥æº', 'æ¥æºè¯¦æƒ…', 'æ¥è‡ªIP', 'æ€»åˆ†']
    df = df.drop(columns=columns_to_drop, errors='ignore')
    #print(df.columns)
    #é‡å‘½ååˆ—å
    #æ•°æ®å­—å…¸ï¼Œç•™æ„æœ‰çš„åˆ—åå­˜åœ¨ä¸åˆç†çš„ç©ºæ ¼
    rename_dict = {
        'æ‚¨çš„æ€§åˆ«?': 'Gender',
        'æ‚¨çš„å¹´é¾„?': 'Age',
        'æ‚¨ç›®å‰çš„èº«ä»½?': 'Identity',
        'æ‚¨çš„æœˆæ¶ˆè´¹é¢æ˜¯?': 'Spending',
        'æ‚¨æ˜¯å¦ä¼šæ„Ÿåˆ°å­¤ç‹¬ï¼Ÿ': 'Loneliness',
        'æ‚¨æ˜¯å¦ä¼šæ„Ÿåˆ°æƒ…ç»ªè¯…ä¸§': 'Depre',
        'å½“æ‚¨éœ€è¦æƒ…æ„Ÿæ”¯æŒæ—¶ï¼Œæ‚¨ä¸»è¦ä¾é ä»€ä¹ˆæ–¹å¼ï¼Ÿ ': 'Support',
        'æ‚¨æ˜¯å¦ä½¿ç”¨è¿‡æƒ…æ„Ÿé™ªä¼´AI?ğŸ’¡ æç¤ºï¼š æƒ…æ„Ÿé™ªä¼´AIæŒ‡çš„æ˜¯èƒ½å¤Ÿä¸ç”¨æˆ·è¿›è¡Œäº’åŠ¨äº¤æµï¼Œå¹¶æä¾›æƒ…æ„Ÿæ”¯æŒæˆ–é™ªä¼´çš„äººå·¥æ™ºèƒ½äº§å“ã€‚ä¾‹å¦‚ï¼š   èŠå¤©æœºå™¨äººï¼ˆå¦‚å°çˆ±åŒå­¦ã€è±†åŒ…æ™ºèƒ½ä½“ã€QQå°å†°ï¼‰  å¿ƒç†é™ªä¼´AIï¼ˆå¦‚æ˜Ÿé‡ã€çŒ«ç®±ï¼‰ æ™ºèƒ½é™ªä¼´åº”ç”¨ï¼ˆå¦‚ Replikaã€ç”·å‹/å¥³å‹AIï¼‰  å…·å¤‡æƒ…æ„Ÿäº’åŠ¨åŠŸèƒ½çš„è™šæ‹Ÿè§’è‰²ï¼ˆå¦‚éƒ¨åˆ†æ¸¸æˆä¸­çš„AIè§’è‰²ã€è™šæ‹Ÿå¶åƒï¼‰': 'UsedFreq',
        'æ‚¨ä½¿ç”¨æƒ…æ„Ÿé™ªä¼´AIçš„ä¸»è¦åŸå› ?(ç¼“è§£å­¤ç‹¬)': 'ReasonLonely',
        '8(å¯»æ±‚æƒ…æ„Ÿå¯„æ‰˜)': 'ReasonEmo',
        '8(è·å–å¿ƒç†æ”¯æŒ)': 'ReasonSupport',
        '8(ç»ƒä¹ ç¤¾äº¤ã€æ²Ÿé€šèƒ½åŠ›)': 'ReasonSocial',
        '8(æ¶ˆé£å¨±ä¹)': 'ReasonEnter',
        '8(æ–°å¥‡ä½“éªŒ)': 'ReasonNovel',
        '8(å…¶ä»–ï¼ˆè¯·è¯´æ˜ï¼‰)': 'ReasonOther',
        'æ‚¨å¯¹æƒ…æ„Ÿé™ªä¼´AIçš„ä½¿ç”¨ä½“éªŒå¦‚ä½•ï¼Ÿâ€”æ•´ä½“ä½“éªŒ': 'ExperOverall',
        'å¯¹è¯è‡ªç„¶åº¦': 'ExperNature',
        'æƒ…æ„Ÿæ”¯æŒåº¦': 'ExperSupport',
        'éšç§ä¿æŠ¤': 'ExperPrivacy',
        'ä¸ªæ€§åŒ–ç¨‹åº¦': 'ExperPersonal',
        'æ‚¨è®¤ä¸ºæƒ…æ„Ÿé™ªä¼´AIä»¥ä¸‹æ–¹é¢çš„é‡è¦ç¨‹åº¦å¦‚ä½•ï¼Ÿâ€”å¯¹è¯è‡ªç„¶åº¦': 'ImporNature',
        'æƒ…æ„Ÿæ”¯æŒåº¦.1': 'ImporSupport',
        'éšç§ä¿æŠ¤.1': 'ImporPrivacy',
        'ä¸ªæ€§åŒ–ç¨‹åº¦.1': 'ImporPersonal',
        'æ‚¨ä½¿ç”¨æƒ…æ„Ÿé™ªä¼´AIçš„ä¸»è¦åœºæ™¯ï¼Ÿ(ç¡å‰æ”¾æ¾)': 'ScenarioSleep',
        '11(å·¥ä½œå­¦ä¹ é—´éš™)': 'ScenarioWork',
        '11(æƒ…ç»ªä½è½æ—¶)': 'ScenarioDown',
        '11(æ—¥å¸¸å¨±ä¹)': 'ScenarioNovel',
        '11(é•¿é€”æ—…è¡Œä¸­)': 'ScenarioTravel',
        '11(å…¶å®ƒ)': 'ScenarioOther',
        'æ‚¨æœªä½¿ç”¨è¿‡æƒ…æ„Ÿé™ªä¼´ AIï¼ŒåŸå› æ˜¯?ï¼ˆå¤šé€‰ï¼‰(ä¸äº†è§£äº§å“)': 'NotReasonProduct',
        '12(è§‰å¾—æ²¡å¿…è¦)': 'NotReasonNece',
        '12(æ‹…å¿ƒéšç§é—®é¢˜)': 'NotReasonPrivacy',
        '12(å…¶ä»–ï¼ˆè¯·è¯´æ˜ï¼‰)': 'NotReasonOther',
        ' æ‚¨å¯¹å°è¯•ä½¿ç”¨æƒ…æ„Ÿé™ªä¼´AIçš„æ„æ„¿å¦‚ä½•?': 'Try',
        'æ‚¨å¯¹æƒ…æ„Ÿé™ªä¼´ AI å¯èƒ½å¸¦æ¥çš„å½±å“æœ‰å“ªäº›æ‹…å¿§?ï¼ˆå¤šé€‰ï¼‰(å½±å“ç°å®ç¤¾äº¤å…³ç³»)': 'ConcernSocial',
        '14(å¼•å‘ä¼¦ç†æˆ–é“å¾·é—®é¢˜)': 'ConcernMoral',
        '14(å­˜åœ¨æ•°æ®éšç§å’Œå®‰å…¨é£é™©)': 'ConcernPrivacy',
        '14(ä½¿ç”¨ä½“éªŒä¸ä½³)': 'ConcernExper',
        '14(æ²¡æœ‰ç‰¹åˆ«æ‹…å¿§)': 'ConcernNot',
        '14(å…¶ä»–ï¼ˆè¯·å¡«å†™ï¼‰)': 'ConcernOther',
        'æ‚¨å¯¹æƒ…æ„Ÿé™ªä¼´AIçš„ä»˜è´¹æ„æ„¿ï¼Ÿ': 'Pay',
        'å¦‚æœæŒ‰æœˆä»˜æ¬¾ï¼Œæ‚¨æœŸæœ›çš„æƒ…æ„Ÿé™ªä¼´AIæœåŠ¡çš„ä»·æ ¼èŒƒå›´æ˜¯å¤šå°‘?': 'Price',
        'å¦‚æœæ„¿æ„ä»˜è´¹ï¼Œæ‚¨æ›´å€¾å‘äºå“ªç§ä»˜è´¹æ¨¡å¼?ï¼ˆå•é€‰ï¼‰': 'PayModel',
        'åœ¨é€‰æ‹©æƒ…æ„Ÿé™ªä¼´AIäº§å“æ—¶ï¼Œå“ªäº›å› ç´ å¯¹æ‚¨æœ€é‡è¦?(å¯¹è¯è´¨é‡)': 'ChooseDialogue',
        '18(ä»·æ ¼)': 'ChoosePrice',
        '18(éšç§ä¿æŠ¤)': 'ChoosePrivacy',
        '18(ä¸ªæ€§åŒ–å®šåˆ¶)': 'ChoosePersonal',
        '18(å“ç‰Œä¿¡èª‰)': 'ChooseBrand',
        '18(ç”¨æˆ·å£ç¢‘)': 'ChooseUser',
        '18(åŠŸèƒ½ä¸°å¯Œæ€§)': 'ChooseFunction',
        '18(å…¶ä»–ï¼ˆè¯·è¯´æ˜ï¼‰)': 'ChooseOther',
        'æ‚¨è®¤ä¸º AI é™ªä¼´çš„å¸‚åœºå‰æ™¯å¦‚ä½•?': 'Market'
    }
    df = df.rename(columns=rename_dict)
    #print(df.columns)
    
    dfuf = df.copy()
    dfkm = df.copy()
    
    ''' ----------------------------------'''
    ''' æ‰€æœ‰ç±»åˆ«å˜é‡åšç‹¬çƒ­ç¼–ç  '''
    
    ohe = OneHotEncoder(drop='first', sparse_output=False)
    categorical_features = ['Gender','Identity','Support','UsedFreq','PayModel']
    
    #print(df[categorical_features].dtypes)
    
    # è½¬æ¢æ•°æ®ç±»å‹
    for col in categorical_features:
        if df[col].dtype != 'object':
            df[col] = df[col].astype(str)
    
    df_encoded = pd.DataFrame(ohe.fit_transform(df[categorical_features]))
    df_encoded.columns = ohe.get_feature_names_out(categorical_features)
    
    df = df.drop(columns=categorical_features)
    df = pd.concat([df, df_encoded], axis=1)
    
    ohe_dict = {
    'Gender_2': 'Gender_Female',
    'Identity_2': 'Identity_graduate',
    'Identity_3': 'Identity_part',
    'Identity_4': 'Identity_work',
    'Support_2': 'Support_network',
    'Support_3': 'Support_self',
    'Support_4': 'Support_friend',
    'UsedFreq_2': 'UsedFreq_sometimes',
    'UsedFreq_3': 'UsedFreq_heard',
    'UsedFreq_4': 'UsedFreq_never',
    'PayModel_2': 'PayModel_use',
    'PayModel_3': 'PayModel_subs',
    'PayModel_4': 'PayModel_purchase',
    'PayModel_5': 'PayModel_other'
    }
    
    df = df.rename(columns = ohe_dict)
    
    
    ''' ----------------------------------'''
    ''' å¡«å……ç©ºå€¼ '''
    dfnn = df.copy()
    fill_scale_0 = [
        "ReasonLonely", "ReasonEmo", "ReasonSupport", "ReasonSocial", "ReasonEnter", 
        "ReasonNovel", "ReasonOther", "ExperOverall", "ExperNature", "ExperSupport", 
        "ExperPrivacy", "ExperPersonal", "ImporNature", "ImporSupport", "ImporPrivacy", 
        "ImporPersonal", "ScenarioSleep", "ScenarioWork", "ScenarioDown", "ScenarioNovel", 
        "ScenarioTravel", "ScenarioOther"
        ]
    dfnn[fill_scale_0] = dfnn[fill_scale_0].fillna(0)

    fill_scale_6 = [
        "NotReasonProduct", "NotReasonNece", "NotReasonPrivacy", "NotReasonOther", "Try"
        ]
    dfnn[fill_scale_6] = dfnn[fill_scale_6].fillna(6)
    

    clean_path = os.path.join(output_path,"cleaned_excel_file.xlsx")
    dfnn.to_excel(clean_path, index=False)
    print(f"æ•°æ®æ¸…æ´—ç»“æœå·²ä¿å­˜åˆ° {clean_path}")
    
    
    
    ''' ----------------------------------'''
    ''' é™¤äº†UsedFreqï¼Œå…¶ä»–åšç‹¬çƒ­ç¼–ç ï¼Œå¹¶å¡«å……ç©ºå€¼ '''
    ohe = OneHotEncoder(drop='first', sparse_output=False)
    categorical_features = ['Gender','Identity','Support','PayModel']
    
    # è½¬æ¢æ•°æ®ç±»å‹
    for col in categorical_features:
        if dfuf[col].dtype != 'object':
            dfuf[col] = dfuf[col].astype(str)
    
    dfuf_encoded = pd.DataFrame(ohe.fit_transform(dfuf[categorical_features]))
    dfuf_encoded.columns = ohe.get_feature_names_out(categorical_features)
    dfuf = dfuf.drop(columns=categorical_features)
    dfuf = pd.concat([dfuf, dfuf_encoded],axis = 1)
    
    ohe_dict = {
    'Gender_2': 'Gender_Female',
    'Identity_2': 'Identity_graduate',
    'Identity_3': 'Identity_part',
    'Identity_4': 'Identity_work',
    'Support_2': 'Support_network',
    'Support_3': 'Support_self',
    'Support_4': 'Support_friend',
    'PayModel_2': 'PayModel_use',
    'PayModel_3': 'PayModel_subs',
    'PayModel_4': 'PayModel_purchase',
    'PayModel_5': 'PayModel_other'
    }
    
    dfuf = dfuf.rename(columns = ohe_dict)
    dfuf['UsedFreq'] = dfuf['UsedFreq'].replace({1: 4, 2: 3, 3: 2, 4: 1})
    
    fill_scale_0 = [
        "ReasonLonely", "ReasonEmo", "ReasonSupport", "ReasonSocial", "ReasonEnter", 
        "ReasonNovel", "ReasonOther", "ExperOverall", "ExperNature", "ExperSupport", 
        "ExperPrivacy", "ExperPersonal", "ImporNature", "ImporSupport", "ImporPrivacy", 
        "ImporPersonal", "ScenarioSleep", "ScenarioWork", "ScenarioDown", "ScenarioNovel", 
        "ScenarioTravel", "ScenarioOther"
        ]
    dfuf[fill_scale_0] = dfuf[fill_scale_0].fillna(0)

    fill_scale_6 = [
        "NotReasonProduct", "NotReasonNece", "NotReasonPrivacy", "NotReasonOther", "Try"
        ]
    dfuf[fill_scale_6] = dfuf[fill_scale_6].fillna(6)
    
    dfuf.to_excel(os.path.join(output_path,"UsedFreq_cleaned_excel_file.xlsx"), index = False)
    
    
    
    ''' ----------------------------------'''
    ''' å¯¹äºKmeansèšç±»ï¼Œç‹¬çƒ­ç¼–ç ä¸dropä¸€ç±»ï¼Œå–å€¼æ ‡å‡†åŒ– '''
    
    ''' ç‹¬çƒ­ç¼–ç  '''
    ohe = OneHotEncoder(drop=None, sparse_output=False)
    categorical_features = ['Gender', 'Identity', 'Support', 'UsedFreq', 'PayModel']
    
    # ç¡®ä¿å…¨æ˜¯å­—ç¬¦ä¸²
    for col in categorical_features:
        if dfkm[col].dtype != 'object':
            dfkm[col] = dfkm[col].astype(str)
    
    dfkm_encoded = pd.DataFrame(ohe.fit_transform(dfkm[categorical_features]))
    dfkm_encoded.columns = ohe.get_feature_names_out(categorical_features)
    
    dfkm = dfkm.drop(columns=categorical_features)
    dfkm = pd.concat([dfkm, dfkm_encoded], axis=1)
    
    ohe_dict = {
        'Gender_1': 'Gender_Male',
        'Gender_2': 'Gender_Female',
        'Identity_1': 'Identity_undergraduate',
        'Identity_2': 'Identity_graduate',
        'Identity_3': 'Identity_part',
        'Identity_4': 'Identity_work',
        'Support_1': 'Support_ai',
        'Support_2': 'Support_network',
        'Support_3': 'Support_self',
        'Support_4': 'Support_friend',
        'UsedFreq_1': 'UsedFreq_often',
        'UsedFreq_2': 'UsedFreq_sometimes',
        'UsedFreq_3': 'UsedFreq_heard',
        'UsedFreq_4': 'UsedFreq_never',
        'PayModel_1': 'PayModel_advertisement',
        'PayModel_2': 'PayModel_use',
        'PayModel_3': 'PayModel_subs',
        'PayModel_4': 'PayModel_purchase',
        'PayModel_5': 'PayModel_other'
        }
    
    dfkm = dfkm.rename(columns = ohe_dict)

    
    ''' å¡«å……ç©ºå€¼ '''
    fill_scale_0 = [
        "ReasonLonely", "ReasonEmo", "ReasonSupport", "ReasonSocial", "ReasonEnter", 
        "ReasonNovel", "ReasonOther", "ExperOverall", "ExperNature", "ExperSupport", 
        "ExperPrivacy", "ExperPersonal", "ImporNature", "ImporSupport", "ImporPrivacy", 
        "ImporPersonal", "ScenarioSleep", "ScenarioWork", "ScenarioDown", "ScenarioNovel", 
        "ScenarioTravel", "ScenarioOther"
    ]
    dfkm[fill_scale_0] = dfkm[fill_scale_0].fillna(0)
    
    fill_scale_6 = [
        "NotReasonProduct", "NotReasonNece", "NotReasonPrivacy", "NotReasonOther", "Try"
    ]
    dfkm[fill_scale_6] = dfkm[fill_scale_6].fillna(6)
    
    ''' å…¨éƒ¨å˜é‡æ ‡å‡†åŒ– (0â€“1) '''
    scaler = MinMaxScaler()
    dfkm = pd.DataFrame(scaler.fit_transform(dfkm), columns=dfkm.columns)
    
    dfkm.to_excel(os.path.join(output_path,"kmeansdata_excel_file.xlsx"), index = False)

    
    
''' ----------------------------------'''
''' å› å­åˆ†æ  '''
# é€‰æ‹©ç”¨äºå› å­åˆ†æçš„åˆ—ï¼ˆæ•°å€¼å‹å˜é‡ï¼Œå¦‚ Likert é‡è¡¨ï¼‰
columns_for_factor_analysis = [
    "Loneliness", "Depre", "ReasonLonely", "ReasonEmo", "ReasonSupport", "ReasonSocial", 
    "ReasonEnter", "ReasonNovel", "ReasonOther", "ExperOverall", "ExperNature", "ExperSupport", 
    "ExperPrivacy", "ExperPersonal", "ImporNature", "ImporSupport", "ImporPrivacy", "ImporPersonal",
    "ScenarioSleep", "ScenarioWork", "ScenarioDown", "ScenarioNovel", "ScenarioTravel", "ScenarioOther",
    "NotReasonProduct", "NotReasonNece", "NotReasonPrivacy", "NotReasonOther", "Try", "ConcernSocial", 
    "ConcernMoral", "ConcernPrivacy", "ConcernExper", "ConcernNot", "ConcernOther", "Pay", "Price", 
    "ChooseDialogue", "ChoosePrice", "ChoosePrivacy", "ChoosePersonal", "ChooseBrand", "ChooseUser",
    "ChooseFunction", "ChooseOther", "Market","Support_network", "Support_self", "Support_friend", 
    "UsedFreq_sometimes", "UsedFreq_heard", "UsedFreq_never", "PayModel_use", "PayModel_subs", 
    "PayModel_purchase", "PayModel_other",
]

df_fa = dfnn[columns_for_factor_analysis]

'''
# åˆ é™¤æ–¹å·®ä¸º0çš„åˆ—
df_fa = df_fa.loc[:, df_fa.var() != 0]
# åˆ é™¤é‡å¤åˆ—
df_fa = df_fa.loc[:, ~df_fa.T.duplicated()]
# åˆ é™¤å«ç¼ºå¤±å€¼çš„è¡Œ
df_fa = df_fa.dropna()

print(df_fa)

# æ‰“å°å«æœ‰ NaN æˆ– Inf çš„è¡Œ
print("åŒ…å« NaN æˆ– Inf çš„è¡Œï¼š")
# æ£€æŸ¥æ˜¯å¦æœ‰ç¼ºå¤±å€¼æˆ–æ— ç©·å¤§å€¼
invalid_rows = df_fa[df_fa.isnull().any(axis=1) | (df_fa.isin([np.inf, -np.inf]).any(axis=1))]
print(invalid_rows)

# æ‰“å°æ¯åˆ—çš„æ•°æ®ç±»å‹
print("æ¯åˆ—çš„æ•°æ®ç±»å‹ï¼š")
print(df_fa.dtypes)
'''

from factor_analyzer import calculate_kmo

# KMO æ£€éªŒ
kmo_all, kmo_model = calculate_kmo(df_fa)
print(f"KMO å€¼: {kmo_model:.3f}")

from scipy.stats import chi2

def bartlett_sphericity_manual(data):
    """
    è®¡ç®— Bartlett çƒå½¢æ£€éªŒ
    """
    n, p = data.shape
    corr_matrix = np.corrcoef(data, rowvar=False) 
    det_corr_matrix = np.linalg.det(corr_matrix)

    chi_square_value = -(n - 1 - (2 * p + 5) / 6) * np.log(det_corr_matrix)
    df = p * (p - 1) / 2  # è‡ªç”±åº¦
    p_value = 1 - chi2.cdf(chi_square_value, df)

    return chi_square_value, p_value

chi_square_value, p_value = bartlett_sphericity_manual(df_fa.to_numpy())
print(f"Bartlett's Test på€¼: {p_value:.5f}")

log_path = os.path.join(output_path, "å› å­åˆ†æçš„æ£€éªŒ.txt")
with open(log_path, 'w', encoding='utf-8') as f:
    f.write(f"KMO å€¼: {kmo_model:.3f}\n")
    f.write(f"Bartlett's Test på€¼: {p_value:.5f}\n")

plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

fa = FactorAnalyzer(n_factors=len(df_fa.columns), rotation=None)
fa.fit(df_fa)

# è®¡ç®—ç‰¹å¾å€¼
ev, v = fa.get_eigenvalues()

# ç”»ç¢çŸ³å›¾
plt.figure(figsize=(8,5))
plt.scatter(range(1, len(df_fa.columns)+1), ev)
plt.plot(range(1, len(df_fa.columns)+1), ev)
plt.xlabel("å› å­æ•°")
plt.ylabel("ç‰¹å¾å€¼")
plt.title("all_ç¢çŸ³å›¾")
plt.grid()
plt.show()

'''
num_factors = 3
'''

while True:
    try:
        num_factors = int(input("æ ¹æ®ç¢çŸ³å›¾ï¼Œè¯·è¾“å…¥ä½ å¸Œæœ›æå–çš„å› å­æ•°é‡ï¼ˆæ•´æ•°ï¼‰ï¼š"))
        if 1 <= num_factors <= len(df_fa.columns):
            break
        else:
            print(f"è¯·è¾“å…¥ 1 åˆ° {len(df_fa.columns)} ä¹‹é—´çš„æ•´æ•°ã€‚")
    except ValueError:
        print("è¯·è¾“å…¥æœ‰æ•ˆæ•´æ•°")


fa = FactorAnalyzer(n_factors=num_factors, rotation="varimax")  # ä½¿ç”¨ Varimax æ—‹è½¬
fa.fit(df_fa)

# å› å­è½½è·çŸ©é˜µ
loadings = pd.DataFrame(fa.loadings_, index=columns_for_factor_analysis)

# å½’ç±»ï¼šæ‰¾å‡ºè½½è·æœ€å¤§ä¸” > 0.4 çš„å› å­
factor_assignments = []
for var, row in loadings.iterrows():
    max_loading = row.max()
    if max_loading > 0.4:
        factor_idx = row.idxmax()  # æ‰¾åˆ°æœ€å¤§è½½è·çš„å› å­ç´¢å¼•
        factor_assignments.append([var, f'Factor {factor_idx + 1}', max_loading])

factor_df = pd.DataFrame(factor_assignments, columns=['Variable', 'Factor', 'Loading'])


factor_df.to_excel(os.path.join(output_path,"all_factor_assignments.xlsx"), index=False)

#print(factor_df)

    


''' Emoåˆ—ç”Ÿæˆï¼ˆæ ¹æ®å› å­åˆ†æç»“æœï¼‰ '''

from sklearn.decomposition import FactorAnalysis
from sklearn.preprocessing import MinMaxScaler

def generate_emo_column(df, var1='Loneliness', var2='Depre', emo_col='Emo'):
    # åˆ¤æ–­ä¸¤ä¸ªå˜é‡æ˜¯å¦éƒ½å­˜åœ¨
    if var1 not in df.columns or var2 not in df.columns:
        print(f"ç¼ºå°‘å˜é‡ {var1} æˆ– {var2}ï¼Œè·³è¿‡å¤„ç†ã€‚")
        return df
    
    corr = df[[var1, var2]].corr().iloc[0, 1]
    print(f"{var1} å’Œ {var2} çš„ç›¸å…³ç³»æ•°ä¸º: {corr:.3f}")
    
    if abs(corr) >= 0.6:
        # å› å­åˆ†æç”Ÿæˆ Emo
        fa = FactorAnalysis(n_components=1)
        emo_scores = fa.fit_transform(df[[var1, var2]])
        df[emo_col] = emo_scores.flatten()
        scaler = MinMaxScaler(feature_range=(1, 5))
        df[emo_col] = scaler.fit_transform(df[[emo_col]])

        print(f"{emo_col}åˆ—ä½¿ç”¨å› å­åˆ†æç”Ÿæˆ")
    else:
        # ç›´æ¥å¹³å‡ç”Ÿæˆ Emo
        df[emo_col] = df[[var1, var2]].mean(axis=1)
        print(f"{emo_col}åˆ—ä½¿ç”¨å¹³å‡å€¼ç”Ÿæˆ")
    
    return df

dfnn = generate_emo_column(dfnn)
dfkm = generate_emo_column(dfkm)
dfuf = generate_emo_column(dfuf)
df = generate_emo_column(df)




''' å›å½’åˆ†æ '''

stata.run('display "hello from stata"')

stata.pdataframe_to_data(dfnn, force = True)

stata.run(f'cd "{output_path}"')

stata.run('asdoc mvreg ImporNature ImporSupport ImporPrivacy ImporPersonal = Age Spending ReasonLonely ReasonEmo ReasonSupport ReasonSocial ReasonEnter ReasonNovel ExperOverall ExperNature ExperSupport ExperPrivacy ExperPersonal ScenarioSleep ScenarioWork ScenarioDown ScenarioNovel ScenarioTravel Pay Price Gender_Female Identity_graduate Identity_part Identity_work Support_network Support_self Support_friend UsedFreq_sometimes Emo, replace save(åŠŸèƒ½é‡è¦æ€§_å›å½’.doc)') 

stata.run('regress ImporPersonal Age Spending ReasonLonely ReasonEmo ReasonSupport ReasonSocial ReasonEnter ReasonNovel ExperOverall ExperNature ExperSupport ExperPrivacy ExperPersonal ScenarioSleep ScenarioWork ScenarioDown ScenarioNovel ScenarioTravel Pay Price Gender_Female Identity_graduate Identity_part Identity_work Support_network Support_self Support_friend UsedFreq_sometimes Emo')
stata.run('predict residuals1, residuals')

stata.run('log using åŠŸèƒ½éœ€æ±‚åˆ†æ_å›å½’æ£€éªŒ.txt, replace text')
stata.run('swilk residuals1')
stata.run('estat hettest')
stata.run('log close')


stata.run('regress ImporNature Age Spending ReasonLonely ReasonEmo ReasonSupport ReasonSocial ReasonEnter ReasonNovel ExperOverall ExperNature ExperSupport ExperPrivacy ExperPersonal ScenarioSleep ScenarioWork ScenarioDown ScenarioNovel ScenarioTravel Pay Price Gender_Female Identity_graduate Identity_part Identity_work Support_network Support_self Support_friend UsedFreq_sometimes Emo')
res = stata.get_ereturn()  # è·å–å›å½’ç»“æœå­—å…¸&#8203;:contentReference[oaicite:1]{index=1}

# 2. æå–ç³»æ•°å’Œåæ–¹å·®çŸ©é˜µ
# e(b) æ˜¯ 1Ã—(p+1) çš„ç³»æ•°è¡Œå‘é‡ï¼ŒåŒ…å«å¸¸æ•°é¡¹
coef_array = np.array(res['e(b)']).flatten()  # å°†å½¢å¦‚[[b0,b1,...]]æ‹‰å¹³
# e(V) æ˜¯ (p+1)Ã—(p+1) çš„åæ–¹å·®çŸ©é˜µ
cov_matrix = np.array(res['e(V)'])
# è·å–è‡ªå˜é‡ååˆ—è¡¨ï¼ˆå‡è®¾å·²çŸ¥æˆ–é€šè¿‡è„šæœ¬ç»´æŠ¤ï¼‰ï¼Œå¦‚ï¼š
var_names = ['Age', 'Spending', 'ReasonLonely', 'ReasonEmo', 'ReasonSupport',
        'ReasonSocial', 'ReasonEnter', 'ReasonNovel', 'ExperOverall', 'ExperNature',
        'ExperSupport', 'ExperPrivacy', 'ExperPersonal', 'ScenarioSleep', 'ScenarioWork',
        'ScenarioDown', 'ScenarioNovel', 'ScenarioTravel', 'Pay', 'Price',
        'Gender_Female', 'Identity_graduate', 'Identity_part', 'Identity_work',
        'Support_network', 'Support_self', 'Support_friend', 'UsedFreq_sometimes', 'Emo','constant'
        ]  # å¯¹åº” e(b) ä¸­çš„ç³»æ•°é¡ºåº
# æ³¨æ„ï¼šç´¢å¼• 0 å¯¹åº”å¸¸æ•°é¡¹ï¼Œåç»­ä¾åºå¯¹åº” x1,x2,...

# 3. è®¡ç®—ç»Ÿè®¡é‡ï¼šæ ‡å‡†è¯¯ã€tå€¼ã€på€¼
ses = np.sqrt(np.diag(cov_matrix))    # ç³»æ•°æ ‡å‡†è¯¯
t_stats = coef_array / ses           # t å€¼
df_resid = res['e(df_r)']            # æ®‹å·®è‡ªç”±åº¦
p_vals = 2 * stats.t.sf(np.abs(t_stats), df_resid)  # åŒå°¾ t æ£€éªŒ p å€¼

# 4. ç­›é€‰æ˜¾è‘—å˜é‡ï¼ˆä¸å«å¸¸æ•°é¡¹ï¼‰
sig_mask = (p_vals < 0.1) & (np.array(var_names) != 'Constant')
sig_idx = np.where(sig_mask)[0]
# æå–æ˜¾è‘—å˜é‡çš„åç§°ã€ç³»æ•°å’Œ p å€¼
sig_names = [var_names[i] for i in sig_idx]
sig_coefs = coef_array[sig_idx]
sig_pvals = p_vals[sig_idx]
# æŒ‰ç³»æ•°å¤§å°æ’åºï¼ˆè¿™é‡ŒæŒ‰ä»å¤§åˆ°å°ï¼‰
order = np.argsort(sig_coefs)[::-1]
sig_names = [sig_names[i] for i in order]
sig_coefs = sig_coefs[order]
sig_pvals = sig_pvals[order]

# 5. ç»˜åˆ¶æ¡å½¢å›¾å’Œæ•£ç‚¹å›¾
plt.rcParams['font.sans-serif'] = ['SimHei']      # è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['axes.unicode_minus'] = False       # æ­£ç¡®æ˜¾ç¤ºè´Ÿå·
fig, ax = plt.subplots(figsize=(6,4))
y_pos = np.arange(len(sig_coefs))

# æ ¹æ®ç³»æ•°æ­£è´Ÿè®¾ç½®æ¡å½¢é¢œè‰²
colors = ['#fdd5b1' if c > 0 else '#f88379' for c in sig_coefs]
# ç»˜åˆ¶æ°´å¹³æ¡å½¢å›¾
ax.barh(y_pos, sig_coefs, color=colors, edgecolor='gray')
ax.set_yticks(y_pos)
ax.set_yticklabels(sig_names, rotation=30, ha='right')  # å˜é‡åå€¾æ–œ30Â°
ax.set_xlabel("å›å½’ç³»æ•°")
ax.set_title("åŠŸèƒ½éœ€æ±‚åˆ†æçš„çº¿æ€§å›å½’ç»“æœï¼ˆä»¥å¯¹è¯è‡ªç„¶åº¦ä¸ºä¾‹ï¼‰")


# æ·»åŠ ç¬¬äºŒåæ ‡è½´ç”¨äº p å€¼æ•£ç‚¹
ax2 = ax.twiny()
ax2.set_xlim(-0.05, 1)  
# ç»˜åˆ¶ p å€¼æ•£ç‚¹
ax2.scatter(sig_pvals, y_pos, color='#00555A', marker='o', s=50, label="p å€¼")
ax2.set_xlabel("P å€¼")

# å›¾ä¾‹
pos_patch = mpatches.Patch(color='#fdd5b1', label='æ­£å‘ç³»æ•°')
neg_patch = mpatches.Patch(color='#f88379', label='è´Ÿå‘ç³»æ•°')
ax.legend(handles=[pos_patch, neg_patch, ax2.collections[0]], loc='upper right')

plt.tight_layout()
plt.savefig(os.path.join(output_path,'åŠŸèƒ½éœ€æ±‚åˆ†æçš„çº¿æ€§å›å½’ç»“æœï¼ˆå¯¹è¯è‡ªç„¶åº¦ï¼‰'), dpi = 300)
#plt.show()

''' -------------------------------------------------------- '''


stata.run('regress ImporSupport Age Spending ReasonLonely ReasonEmo ReasonSupport ReasonSocial ReasonEnter ReasonNovel ExperOverall ExperNature ExperSupport ExperPrivacy ExperPersonal ScenarioSleep ScenarioWork ScenarioDown ScenarioNovel ScenarioTravel Pay Price Gender_Female Identity_graduate Identity_part Identity_work Support_network Support_self Support_friend UsedFreq_sometimes Emo')
res = stata.get_ereturn()  # è·å–å›å½’ç»“æœå­—å…¸&#8203;:contentReference[oaicite:1]{index=1}

# 2. æå–ç³»æ•°å’Œåæ–¹å·®çŸ©é˜µ
# e(b) æ˜¯ 1Ã—(p+1) çš„ç³»æ•°è¡Œå‘é‡ï¼ŒåŒ…å«å¸¸æ•°é¡¹
coef_array = np.array(res['e(b)']).flatten()  # å°†å½¢å¦‚[[b0,b1,...]]æ‹‰å¹³
# e(V) æ˜¯ (p+1)Ã—(p+1) çš„åæ–¹å·®çŸ©é˜µ
cov_matrix = np.array(res['e(V)'])
# è·å–è‡ªå˜é‡ååˆ—è¡¨ï¼ˆå‡è®¾å·²çŸ¥æˆ–é€šè¿‡è„šæœ¬ç»´æŠ¤ï¼‰ï¼Œå¦‚ï¼š
var_names = ['Age', 'Spending', 'ReasonLonely', 'ReasonEmo', 'ReasonSupport',
        'ReasonSocial', 'ReasonEnter', 'ReasonNovel', 'ExperOverall', 'ExperNature',
        'ExperSupport', 'ExperPrivacy', 'ExperPersonal', 'ScenarioSleep', 'ScenarioWork',
        'ScenarioDown', 'ScenarioNovel', 'ScenarioTravel', 'Pay', 'Price',
        'Gender_Female', 'Identity_graduate', 'Identity_part', 'Identity_work',
        'Support_network', 'Support_self', 'Support_friend', 'UsedFreq_sometimes', 'Emo','constant'
        ]  # å¯¹åº” e(b) ä¸­çš„ç³»æ•°é¡ºåº
# æ³¨æ„ï¼šç´¢å¼• 0 å¯¹åº”å¸¸æ•°é¡¹ï¼Œåç»­ä¾åºå¯¹åº” x1,x2,...

# 3. è®¡ç®—ç»Ÿè®¡é‡ï¼šæ ‡å‡†è¯¯ã€tå€¼ã€på€¼
ses = np.sqrt(np.diag(cov_matrix))    # ç³»æ•°æ ‡å‡†è¯¯
t_stats = coef_array / ses           # t å€¼
df_resid = res['e(df_r)']            # æ®‹å·®è‡ªç”±åº¦
p_vals = 2 * stats.t.sf(np.abs(t_stats), df_resid)  # åŒå°¾ t æ£€éªŒ p å€¼

# 4. ç­›é€‰æ˜¾è‘—å˜é‡ï¼ˆä¸å«å¸¸æ•°é¡¹ï¼‰
sig_mask = (p_vals < 0.1) & (np.array(var_names) != 'Constant')
sig_idx = np.where(sig_mask)[0]
# æå–æ˜¾è‘—å˜é‡çš„åç§°ã€ç³»æ•°å’Œ p å€¼
sig_names = [var_names[i] for i in sig_idx]
sig_coefs = coef_array[sig_idx]
sig_pvals = p_vals[sig_idx]
# æŒ‰ç³»æ•°å¤§å°æ’åºï¼ˆè¿™é‡ŒæŒ‰ä»å¤§åˆ°å°ï¼‰
order = np.argsort(sig_coefs)[::-1]
sig_names = [sig_names[i] for i in order]
sig_coefs = sig_coefs[order]
sig_pvals = sig_pvals[order]

# 5. ç»˜åˆ¶æ¡å½¢å›¾å’Œæ•£ç‚¹å›¾
plt.rcParams['font.sans-serif'] = ['SimHei']      # è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['axes.unicode_minus'] = False       # æ­£ç¡®æ˜¾ç¤ºè´Ÿå·
fig, ax = plt.subplots(figsize=(6,4))
y_pos = np.arange(len(sig_coefs))

# æ ¹æ®ç³»æ•°æ­£è´Ÿè®¾ç½®æ¡å½¢é¢œè‰²
colors = ['#fdd5b1' if c > 0 else '#f88379' for c in sig_coefs]
# ç»˜åˆ¶æ°´å¹³æ¡å½¢å›¾
ax.barh(y_pos, sig_coefs, color=colors, edgecolor='gray')
ax.set_yticks(y_pos)
ax.set_yticklabels(sig_names, rotation=30, ha='right')  # å˜é‡åå€¾æ–œ30Â°
ax.set_xlabel("å›å½’ç³»æ•°")
ax.set_title("åŠŸèƒ½éœ€æ±‚åˆ†æçš„çº¿æ€§å›å½’ç»“æœï¼ˆä»¥æƒ…æ„Ÿæ”¯æŒä¸ºä¾‹ï¼‰")

# æ·»åŠ ç¬¬äºŒåæ ‡è½´ç”¨äº p å€¼æ•£ç‚¹
ax2 = ax.twiny()
ax2.set_xlim(-0.05, 1)  
# ç»˜åˆ¶ p å€¼æ•£ç‚¹
ax2.scatter(sig_pvals, y_pos, color='#00555A', marker='o', s=50, label="p å€¼")
ax2.set_xlabel("P å€¼")

# å›¾ä¾‹
pos_patch = mpatches.Patch(color='#fdd5b1', label='æ­£å‘ç³»æ•°')
neg_patch = mpatches.Patch(color='#f88379', label='è´Ÿå‘ç³»æ•°')
ax.legend(handles=[pos_patch, neg_patch, ax2.collections[0]], loc='upper right')

plt.tight_layout()
plt.savefig(os.path.join(output_path,'åŠŸèƒ½éœ€æ±‚åˆ†æçš„çº¿æ€§å›å½’ç»“æœï¼ˆæƒ…æ„Ÿæ”¯æŒï¼‰'), dpi = 300)
#plt.show()


''' -------------------------------------------------------- '''


stata.run('regress ImporPrivacy Age Spending ReasonLonely ReasonEmo ReasonSupport ReasonSocial ReasonEnter ReasonNovel ExperOverall ExperNature ExperSupport ExperPrivacy ExperPersonal ScenarioSleep ScenarioWork ScenarioDown ScenarioNovel ScenarioTravel Pay Price Gender_Female Identity_graduate Identity_part Identity_work Support_network Support_self Support_friend UsedFreq_sometimes Emo')
res = stata.get_ereturn()  # è·å–å›å½’ç»“æœå­—å…¸&#8203;:contentReference[oaicite:1]{index=1}

# 2. æå–ç³»æ•°å’Œåæ–¹å·®çŸ©é˜µ
# e(b) æ˜¯ 1Ã—(p+1) çš„ç³»æ•°è¡Œå‘é‡ï¼ŒåŒ…å«å¸¸æ•°é¡¹
coef_array = np.array(res['e(b)']).flatten()  # å°†å½¢å¦‚[[b0,b1,...]]æ‹‰å¹³
# e(V) æ˜¯ (p+1)Ã—(p+1) çš„åæ–¹å·®çŸ©é˜µ
cov_matrix = np.array(res['e(V)'])
# è·å–è‡ªå˜é‡ååˆ—è¡¨ï¼ˆå‡è®¾å·²çŸ¥æˆ–é€šè¿‡è„šæœ¬ç»´æŠ¤ï¼‰ï¼Œå¦‚ï¼š
var_names = ['Age', 'Spending', 'ReasonLonely', 'ReasonEmo', 'ReasonSupport',
        'ReasonSocial', 'ReasonEnter', 'ReasonNovel', 'ExperOverall', 'ExperNature',
        'ExperSupport', 'ExperPrivacy', 'ExperPersonal', 'ScenarioSleep', 'ScenarioWork',
        'ScenarioDown', 'ScenarioNovel', 'ScenarioTravel', 'Pay', 'Price',
        'Gender_Female', 'Identity_graduate', 'Identity_part', 'Identity_work',
        'Support_network', 'Support_self', 'Support_friend', 'UsedFreq_sometimes', 'Emo','constant'
        ]  # å¯¹åº” e(b) ä¸­çš„ç³»æ•°é¡ºåº
# æ³¨æ„ï¼šç´¢å¼• 0 å¯¹åº”å¸¸æ•°é¡¹ï¼Œåç»­ä¾åºå¯¹åº” x1,x2,...

# 3. è®¡ç®—ç»Ÿè®¡é‡ï¼šæ ‡å‡†è¯¯ã€tå€¼ã€på€¼
ses = np.sqrt(np.diag(cov_matrix))    # ç³»æ•°æ ‡å‡†è¯¯
t_stats = coef_array / ses           # t å€¼
df_resid = res['e(df_r)']            # æ®‹å·®è‡ªç”±åº¦
p_vals = 2 * stats.t.sf(np.abs(t_stats), df_resid)  # åŒå°¾ t æ£€éªŒ p å€¼

# 4. ç­›é€‰æ˜¾è‘—å˜é‡ï¼ˆä¸å«å¸¸æ•°é¡¹ï¼‰
sig_mask = (p_vals < 0.1) & (np.array(var_names) != 'Constant')
sig_idx = np.where(sig_mask)[0]
# æå–æ˜¾è‘—å˜é‡çš„åç§°ã€ç³»æ•°å’Œ p å€¼
sig_names = [var_names[i] for i in sig_idx]
sig_coefs = coef_array[sig_idx]
sig_pvals = p_vals[sig_idx]
# æŒ‰ç³»æ•°å¤§å°æ’åºï¼ˆè¿™é‡ŒæŒ‰ä»å¤§åˆ°å°ï¼‰
order = np.argsort(sig_coefs)[::-1]
sig_names = [sig_names[i] for i in order]
sig_coefs = sig_coefs[order]
sig_pvals = sig_pvals[order]

# 5. ç»˜åˆ¶æ¡å½¢å›¾å’Œæ•£ç‚¹å›¾
plt.rcParams['font.sans-serif'] = ['SimHei']      # è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['axes.unicode_minus'] = False       # æ­£ç¡®æ˜¾ç¤ºè´Ÿå·
fig, ax = plt.subplots(figsize=(6,4))
y_pos = np.arange(len(sig_coefs))

# æ ¹æ®ç³»æ•°æ­£è´Ÿè®¾ç½®æ¡å½¢é¢œè‰²
colors = ['#fdd5b1' if c > 0 else '#f88379' for c in sig_coefs]
# ç»˜åˆ¶æ°´å¹³æ¡å½¢å›¾
ax.barh(y_pos, sig_coefs, color=colors, edgecolor='gray')
ax.set_yticks(y_pos)
ax.set_yticklabels(sig_names, rotation=30, ha='right')  # å˜é‡åå€¾æ–œ30Â°
ax.set_xlabel("å›å½’ç³»æ•°")
ax.set_title("åŠŸèƒ½éœ€æ±‚åˆ†æçš„çº¿æ€§å›å½’ç»“æœï¼ˆä»¥éšç§ä¿æŠ¤ä¸ºä¾‹ï¼‰")

# æ·»åŠ ç¬¬äºŒåæ ‡è½´ç”¨äº p å€¼æ•£ç‚¹
ax2 = ax.twiny()
ax2.set_xlim(-0.05, 1)  
# ç»˜åˆ¶ p å€¼æ•£ç‚¹
ax2.scatter(sig_pvals, y_pos, color='#00555A', marker='o', s=50, label="p å€¼")
ax2.set_xlabel("P å€¼")

# å›¾ä¾‹
pos_patch = mpatches.Patch(color='#fdd5b1', label='æ­£å‘ç³»æ•°')
neg_patch = mpatches.Patch(color='#f88379', label='è´Ÿå‘ç³»æ•°')
ax.legend(handles=[pos_patch, neg_patch, ax2.collections[0]], loc='upper right')

plt.tight_layout()
plt.savefig(os.path.join(output_path,'åŠŸèƒ½éœ€æ±‚åˆ†æçš„çº¿æ€§å›å½’ç»“æœï¼ˆéšç§ä¿æŠ¤ï¼‰'), dpi = 300)
#plt.show()    


''' -------------------------------------------------------- '''

stata.run('regress ImporPersonal Age Spending ReasonLonely ReasonEmo ReasonSupport ReasonSocial ReasonEnter ReasonNovel ExperOverall ExperNature ExperSupport ExperPrivacy ExperPersonal ScenarioSleep ScenarioWork ScenarioDown ScenarioNovel ScenarioTravel Pay Price Gender_Female Identity_graduate Identity_part Identity_work Support_network Support_self Support_friend UsedFreq_sometimes Emo')
res = stata.get_ereturn()  # è·å–å›å½’ç»“æœå­—å…¸&#8203;:contentReference[oaicite:1]{index=1}

# 2. æå–ç³»æ•°å’Œåæ–¹å·®çŸ©é˜µ
# e(b) æ˜¯ 1Ã—(p+1) çš„ç³»æ•°è¡Œå‘é‡ï¼ŒåŒ…å«å¸¸æ•°é¡¹
coef_array = np.array(res['e(b)']).flatten()  # å°†å½¢å¦‚[[b0,b1,...]]æ‹‰å¹³
# e(V) æ˜¯ (p+1)Ã—(p+1) çš„åæ–¹å·®çŸ©é˜µ
cov_matrix = np.array(res['e(V)'])
# è·å–è‡ªå˜é‡ååˆ—è¡¨ï¼ˆå‡è®¾å·²çŸ¥æˆ–é€šè¿‡è„šæœ¬ç»´æŠ¤ï¼‰ï¼Œå¦‚ï¼š
var_names = ['Age', 'Spending', 'ReasonLonely', 'ReasonEmo', 'ReasonSupport',
        'ReasonSocial', 'ReasonEnter', 'ReasonNovel', 'ExperOverall', 'ExperNature',
        'ExperSupport', 'ExperPrivacy', 'ExperPersonal', 'ScenarioSleep', 'ScenarioWork',
        'ScenarioDown', 'ScenarioNovel', 'ScenarioTravel', 'Pay', 'Price',
        'Gender_Female', 'Identity_graduate', 'Identity_part', 'Identity_work',
        'Support_network', 'Support_self', 'Support_friend', 'UsedFreq_sometimes', 'Emo', 'constant'
        ]  # å¯¹åº” e(b) ä¸­çš„ç³»æ•°é¡ºåº
# æ³¨æ„ï¼šç´¢å¼• 0 å¯¹åº”å¸¸æ•°é¡¹ï¼Œåç»­ä¾åºå¯¹åº” x1,x2,...

# 3. è®¡ç®—ç»Ÿè®¡é‡ï¼šæ ‡å‡†è¯¯ã€tå€¼ã€på€¼
ses = np.sqrt(np.diag(cov_matrix))    # ç³»æ•°æ ‡å‡†è¯¯
t_stats = coef_array / ses           # t å€¼
df_resid = res['e(df_r)']            # æ®‹å·®è‡ªç”±åº¦
p_vals = 2 * stats.t.sf(np.abs(t_stats), df_resid)  # åŒå°¾ t æ£€éªŒ p å€¼

# 4. ç­›é€‰æ˜¾è‘—å˜é‡ï¼ˆä¸å«å¸¸æ•°é¡¹ï¼‰
sig_mask = (p_vals < 0.1) & (np.array(var_names) != 'Constant')
sig_idx = np.where(sig_mask)[0]
# æå–æ˜¾è‘—å˜é‡çš„åç§°ã€ç³»æ•°å’Œ p å€¼
sig_names = [var_names[i] for i in sig_idx]
sig_coefs = coef_array[sig_idx]
sig_pvals = p_vals[sig_idx]
# æŒ‰ç³»æ•°å¤§å°æ’åºï¼ˆè¿™é‡ŒæŒ‰ä»å¤§åˆ°å°ï¼‰
order = np.argsort(sig_coefs)[::-1]
sig_names = [sig_names[i] for i in order]
sig_coefs = sig_coefs[order]
sig_pvals = sig_pvals[order]

# 5. ç»˜åˆ¶æ¡å½¢å›¾å’Œæ•£ç‚¹å›¾
plt.rcParams['font.sans-serif'] = ['SimHei']      # è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['axes.unicode_minus'] = False       # æ­£ç¡®æ˜¾ç¤ºè´Ÿå·
fig, ax = plt.subplots(figsize=(6,4))
y_pos = np.arange(len(sig_coefs))

# æ ¹æ®ç³»æ•°æ­£è´Ÿè®¾ç½®æ¡å½¢é¢œè‰²
colors = ['#fdd5b1' if c > 0 else '#f88379' for c in sig_coefs]
# ç»˜åˆ¶æ°´å¹³æ¡å½¢å›¾
ax.barh(y_pos, sig_coefs, color=colors, edgecolor='gray')
ax.set_yticks(y_pos)
ax.set_yticklabels(sig_names, rotation=30, ha='right')  # å˜é‡åå€¾æ–œ30Â°
ax.set_xlabel("å›å½’ç³»æ•°")
ax.set_title("åŠŸèƒ½éœ€æ±‚åˆ†æçš„çº¿æ€§å›å½’ç»“æœï¼ˆä»¥ä¸ªæ€§åŒ–å®šåˆ¶ä¸ºä¾‹ï¼‰")

# æ·»åŠ ç¬¬äºŒåæ ‡è½´ç”¨äº p å€¼æ•£ç‚¹
ax2 = ax.twiny()
ax2.set_xlim(-0.05, 1)  
# ç»˜åˆ¶ p å€¼æ•£ç‚¹
ax2.scatter(sig_pvals, y_pos, color='#00555A', marker='o', s=50, label="p å€¼")
ax2.set_xlabel("P å€¼")

pos_patch = mpatches.Patch(color='#fdd5b1', label='æ­£å‘ç³»æ•°')
neg_patch = mpatches.Patch(color='#f88379', label='è´Ÿå‘ç³»æ•°')
ax.legend(handles=[pos_patch, neg_patch, ax2.collections[0]], loc='upper right')

plt.tight_layout()
plt.savefig(os.path.join(output_path,'åŠŸèƒ½éœ€æ±‚åˆ†æçš„çº¿æ€§å›å½’ç»“æœï¼ˆä¸ªæ€§åŒ–å®šåˆ¶åŠŸèƒ½ï¼‰'), dpi = 300)
#plt.show()


''' -------------------------------------------------------- '''

''' æ•´ä½“ä½“éªŒçš„å›å½’ '''

stata.pdataframe_to_data(dfnn, force=True)
stata.run('asdoc ologit ExperOverall ExperNature ExperSupport ExperPrivacy ExperPersonal,  replace save(æ•´ä½“ä½“éªŒ_å›å½’.doc)')
res = stata.get_ereturn()

b = res['e(b)'].flatten()
V = res['e(V)']

variables = ["ExperNature", "ExperSupport", "ExperPrivacy", "ExperPersonal"]
coefs = b[:len(variables)]
se = np.sqrt(np.diag(V))[:len(variables)]
t_vals = coefs / se
p_vals = 2 * (1 - norm.cdf(np.abs(t_vals)))

# æ„å»ºç»“æœè¡¨å¹¶ç­›é€‰æ˜¾è‘—å˜é‡
results_df = pd.DataFrame({
    'coef': coefs,
    'se': se,
    't': t_vals,
    'p': p_vals
}, index=variables)
sig_df = results_df[results_df['p'] < 0.1]
sig_df = sig_df.sort_values('coef', ascending=False)

# å¯è§†åŒ–è®¾ç½®
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

fig, ax = plt.subplots(figsize=(6, 4))
y_pos = np.arange(len(sig_df))

# æ¡å½¢é¢œè‰²
colors = ['#fdd5b1' if c > 0 else '#f88379' for c in sig_df['coef']]
ax.barh(y_pos, sig_df['coef'], color=colors, edgecolor='gray')
ax.set_yticks(y_pos)
ax.set_yticklabels(sig_df.index, rotation=30, ha='right')
ax.set_xlabel("å›å½’ç³»æ•°")
ax.set_title("æ•´ä½“ä½“éªŒçš„æœ‰åºå›å½’ç»“æœ")

# æ·»åŠ åŒåæ ‡è½´ç»˜åˆ¶ p å€¼
ax2 = ax.twiny()
ax2.set_xlim(-0.05, 1)
ax2.scatter(sig_df['p'], y_pos, color='#00555A', marker='o', s=50)
ax2.set_xlabel("P å€¼")

# å›¾ä¾‹è®¾ç½®
pos_patch = mpatches.Patch(color='#fdd5b1', label='æ­£å‘ç³»æ•°')
neg_patch = mpatches.Patch(color='#f88379', label='è´Ÿå‘ç³»æ•°')
pval_patch = mpatches.Patch(color='#00555A', label='på€¼')
ax.legend(handles=[pos_patch, neg_patch, pval_patch], loc='upper right')

# ä¿å­˜å¹¶å±•ç¤º
plt.tight_layout()
plt.savefig("C:/Users/Lucius/Desktop/output/æ•´ä½“ä½“éªŒ_å›å½’ç»“æœ.png", dpi=300)
#plt.show()

stata.run('regress ExperOverall ExperNature ExperSupport ExperPrivacy ExperPersonal')
stata.run('log using æ•´ä½“ä½“éªŒ_å›å½’æ£€éªŒ.txt, replace text')
stata.run('vif')
stata.run('log close')


''' -------------------------------------------------------- '''

''' ä»˜è´¹æ„æ„¿çš„å›å½’ '''

stata.pdataframe_to_data(dfnn, force=True)
stata.run(f'cd "{output_path}"')

stata.run('asdoc regress Pay Age Spending Loneliness Depre UsedFreq_sometimes UsedFreq_heard UsedFreq_never ReasonLonely ReasonEmo ReasonSupport ReasonSocial ReasonEnter ReasonNovel ExperOverall ExperNature ExperSupport ExperPrivacy ExperPersonal ScenarioSleep ScenarioWork ScenarioDown ScenarioNovel ScenarioTravel NotReasonProduct NotReasonNece NotReasonPrivacy Try Gender_Female Identity_graduate Identity_part Identity_work, replace save(ä»˜è´¹æ„æ„¿_å›å½’.doc)')
 
stata.run('regress Pay Age Spending Loneliness Depre UsedFreq_sometimes UsedFreq_heard UsedFreq_never ReasonLonely ReasonEmo ReasonSupport ReasonSocial ReasonEnter ReasonNovel ExperOverall ExperNature ExperSupport ExperPrivacy ExperPersonal ScenarioSleep ScenarioWork ScenarioDown ScenarioNovel ScenarioTravel NotReasonProduct NotReasonNece NotReasonPrivacy Try Gender_Female Identity_graduate Identity_part Identity_work')
stata.run('predict resid_pay, residual')

stata.run('log using ä»˜è´¹æ„æ„¿_å›å½’æ£€éªŒ.txt, replace text')
stata.run('swilk resid_pay')
stata.run('estat hettest')
stata.run('log close')

# Python æå–å›å½’ç»“æœ
res = stata.get_ereturn()
coef_array = np.array(res['e(b)']).flatten()
cov_matrix = np.array(res['e(V)'])
df_resid = res['e(df_r)']

var_names = ['Age', 'Spending', 'Loneliness', 'Depre', 'UsedFreq_sometimes', 'UsedFreq_heard','UsedFreq_never', 
             'ReasonLonely', 'ReasonEmo', 'ReasonSupport', 'ReasonSocial', 'ReasonEnter', 'ReasonNovel',
             'ExperOverall', 'ExperNature', 'ExperSupport', 'ExperPrivacy', 'ExperPersonal',
             'ScenarioSleep', 'ScenarioWork', 'ScenarioDown', 'ScenarioNovel', 'ScenarioTravel',
             'NotReasonProduct', 'NotReasonNece', 'NotReasonPrivacy', 'Try', 'Gender_Female',
             'Identity_graduate', 'Identity_part', 'Identity_work','Constant']

ses = np.sqrt(np.diag(cov_matrix))
t_stats = coef_array / ses
p_vals = 2 * stats.t.sf(np.abs(t_stats), df_resid)

sig_mask = (p_vals < 0.1) & (np.array(var_names) != 'Constant')
sig_idx = np.where(sig_mask)[0]
sig_names = [var_names[i] for i in sig_idx]
sig_coefs = coef_array[sig_idx]
sig_pvals = p_vals[sig_idx]

order = np.argsort(sig_coefs)[::-1]
sig_names = [sig_names[i] for i in order]
sig_coefs = sig_coefs[order]
sig_pvals = sig_pvals[order]

# å¯è§†åŒ–
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False
fig, ax = plt.subplots(figsize=(6, 4))
y_pos = np.arange(len(sig_coefs))
colors = ['#fdd5b1' if c > 0 else '#f88379' for c in sig_coefs]
ax.barh(y_pos, sig_coefs, color=colors, edgecolor='gray')
ax.set_yticks(y_pos)
ax.set_yticklabels(sig_names, rotation=30, ha='right')
ax.set_xlabel("å›å½’ç³»æ•°")
ax.set_title("ä»˜è´¹æ„æ„¿åˆ†æçš„å›å½’ç»“æœ")

ax2 = ax.twiny()
ax2.set_xlim(-0.05, 1)
ax2.scatter(sig_pvals, y_pos, color='#00555A', marker='o', s=50, label="p å€¼")
ax2.set_xlabel("P å€¼")

pos_patch = mpatches.Patch(color='#fdd5b1', label='æ­£å‘ç³»æ•°')
neg_patch = mpatches.Patch(color='#f88379', label='è´Ÿå‘ç³»æ•°')
ax.legend(handles=[pos_patch, neg_patch, ax2.collections[0]], loc='upper right')

plt.tight_layout()
plt.savefig(os.path.join(output_path, 'ä»˜è´¹æ„æ„¿åˆ†æçš„å›å½’ç»“æœ.png'), dpi=300)
#plt.show()

''' -------------------------------------------------------- '''

''' ä½¿ç”¨é¢‘ç‡çš„å›å½’ '''

stata.pdataframe_to_data(dfuf, force=True)
stata.run('asdoc ologit UsedFreq Age Spending Gender_Female Identity_graduate Identity_part Identity_work Loneliness Depre ReasonLonely ReasonEmo ReasonSupport ReasonSocial ReasonEnter ReasonNovel,  replace save(ä½¿ç”¨é¢‘ç‡_å›å½’.doc)')
res = stata.get_ereturn()

b = res['e(b)'].flatten()
V = res['e(V)']

variables = ['Age', 'Spending', 'Gender_Female', 'Identity_graduate', 'Identity_part', 'Identity_work', 
      'Loneliness', 'Depre', 'ReasonLonely', 'ReasonEmo', 'ReasonSupport', 'ReasonSocial', 
      'ReasonEnter', 'ReasonNovel']
coefs = b[:len(variables)]
se = np.sqrt(np.diag(V))[:len(variables)]
t_vals = coefs / se
p_vals = 2 * (1 - norm.cdf(np.abs(t_vals)))

# æ„å»ºç»“æœè¡¨å¹¶ç­›é€‰æ˜¾è‘—å˜é‡
results_df = pd.DataFrame({
    'coef': coefs,
    'se': se,
    't': t_vals,
    'p': p_vals
}, index=variables)
sig_df = results_df[results_df['p'] < 0.1]
sig_df = sig_df.sort_values('coef', ascending=False)

# å¯è§†åŒ–è®¾ç½®
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

fig, ax = plt.subplots(figsize=(6, 4))
y_pos = np.arange(len(sig_df))

# æ¡å½¢é¢œè‰²
colors = ['#fdd5b1' if c > 0 else '#f88379' for c in sig_df['coef']]
ax.barh(y_pos, sig_df['coef'], color=colors, edgecolor='gray')
ax.set_yticks(y_pos)
ax.set_yticklabels(sig_df.index, rotation=30, ha='right')
ax.set_xlabel("å›å½’ç³»æ•°")
ax.set_title("ä½¿ç”¨é¢‘ç‡çš„æœ‰åºå›å½’ç»“æœ")

# æ·»åŠ åŒåæ ‡è½´ç»˜åˆ¶ p å€¼
ax2 = ax.twiny()
ax2.set_xlim(-0.05, 1)
ax2.scatter(sig_df['p'], y_pos, color='#00555A', marker='o', s=50)
ax2.set_xlabel("P å€¼")

# å›¾ä¾‹è®¾ç½®
pos_patch = mpatches.Patch(color='#fdd5b1', label='æ­£å‘ç³»æ•°')
neg_patch = mpatches.Patch(color='#f88379', label='è´Ÿå‘ç³»æ•°')
pval_patch = mpatches.Patch(color='#00555A', label='på€¼')
ax.legend(handles=[pos_patch, neg_patch, pval_patch], loc='upper right')

# ä¿å­˜å¹¶å±•ç¤º
plt.tight_layout()
plt.savefig("C:/Users/Lucius/Desktop/output/ä½¿ç”¨é¢‘ç‡_å›å½’ç»“æœ.png", dpi=300)
#plt.show()

stata.run('regress UsedFreq Age Spending Gender_Female Identity_graduate Identity_part Identity_work Loneliness Depre ReasonLonely ReasonEmo ReasonSupport ReasonSocial ReasonEnter ReasonNovel')
stata.run('log using ä½¿ç”¨é¢‘ç‡_å›å½’æ£€éªŒ.txt, replace text')
stata.run('vif')
stata.run('log close')
    
''' ----------------------------------'''
''' Kmeansèšç±»åˆ†æ '''

plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

''' è‚˜éƒ¨æ³• (SSE) '''
K_range = range(1, 11)
sse = []
for k in K_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(dfkm)
    sse.append(kmeans.inertia_)

plt.figure(figsize=(8, 5))
plt.plot(K_range, sse, marker='o', linestyle='-')
plt.xlabel('ç°‡çš„ä¸ªæ•° K')
plt.ylabel('è¯¯å·®å¹³æ–¹å’Œ (SSE)')
plt.title('è‚˜éƒ¨æ³•ç¡®å®šæœ€ä½³ K å€¼')
plt.xticks(K_range)
plt.grid()
plt.savefig(os.path.join(output_path, 'è‚˜éƒ¨æ³•_æœ€ä½³Kå€¼.png'))
plt.show()
plt.close()

''' è®¡ç®—è½®å»“ç³»æ•° '''
silhouette_scores = []
K_range = range(2, 11)
for K in K_range:
    kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)
    labels = kmeans.fit_predict(dfkm)
    score = silhouette_score(dfkm, labels)
    silhouette_scores.append(score)

plt.figure(figsize=(8, 5))
plt.plot(K_range, silhouette_scores, marker='o', linestyle='-')
plt.xlabel('èšç±»æ•° K')
plt.ylabel('è½®å»“ç³»æ•°')
plt.title('ä¸åŒ K å€¼çš„è½®å»“ç³»æ•°')
plt.grid()
plt.savefig(os.path.join(output_path, 'è½®å»“ç³»æ•°_ä¸åŒKå€¼.png'))
plt.show()
plt.close()

''' è®©ç”¨æˆ·è¾“å…¥Kå€¼ '''
while True:
    try:
        user_K = int(input("è¯·æ ¹æ®è‚˜éƒ¨å›¾å’Œè½®å»“ç³»æ•°å›¾é€‰æ‹©æœ€ç»ˆèšç±»ä¸ªæ•° K: "))
        if 1 <= user_K <= 10:
            break
        else:
            print("è¯·è¾“å…¥1åˆ°10ä¹‹é—´çš„æ•´æ•°ã€‚")
    except ValueError:
        print("è¯·è¾“å…¥æœ‰æ•ˆæ•´æ•°")


''' èšç±» '''
kmeans = KMeans(n_clusters=user_K, random_state=42, n_init=10)
dfkm['Cluster'] = kmeans.fit_predict(dfkm)

dfkm.to_excel(os.path.join(output_path, "èšç±»ç»“æœ.xlsx"), index=False)

''' è®¡ç®—å¹¶ä¿å­˜èšç±»ä¸­å¿ƒã€æ ·æœ¬æ•°é‡ã€å„ç°‡å‡å€¼ '''
centroids = kmeans.cluster_centers_
unique, counts = np.unique(kmeans.labels_, return_counts=True)
cluster_sizes = dict(zip(unique, counts))

df_centroids = pd.DataFrame(centroids, columns=[f'Feature_{i+1}' for i in range(centroids.shape[1])])
df_centroids.to_excel(os.path.join(output_path, 'èšç±»ä¸­å¿ƒ.xlsx'), index=False)

df_cluster_sizes = pd.DataFrame(list(cluster_sizes.items()), columns=['Cluster', 'Sample_Count'])
df_cluster_sizes.to_excel(os.path.join(output_path, 'æ¯ä¸ªç°‡çš„æ ·æœ¬æ•°é‡.xlsx'), index=False)

cluster_means = dfkm.groupby('Cluster').mean(numeric_only=True).round(2).reset_index()
cluster_means.to_excel(os.path.join(output_path, 'å„ç°‡æ ·æœ¬å‡å€¼.xlsx'), index=False)

''' PCA 2ç»´å¯è§†åŒ– '''
pca = PCA(n_components=2)
df_pca = pca.fit_transform(dfkm.drop('Cluster', axis=1))

plt.figure(figsize=(8, 6))
plt.scatter(df_pca[:, 0], df_pca[:, 1], c=dfkm['Cluster'], cmap='viridis', alpha=0.7)
plt.title('KMeans èšç±»ç»“æœ (PCA_2Dé™ç»´)')
plt.xlabel('ä¸»æˆåˆ† 1')
plt.ylabel('ä¸»æˆåˆ† 2')
plt.colorbar(label='Cluster')
plt.savefig(os.path.join(output_path, 'KMeans_PCA_2D.png'))
#plt.show()
plt.close()

''' PCA 3ç»´å¯è§†åŒ– '''
pca_3d = PCA(n_components=3)
df_pca_3d = pca_3d.fit_transform(dfkm.drop('Cluster', axis=1))

fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')
sc = ax.scatter(df_pca_3d[:, 0], df_pca_3d[:, 1], df_pca_3d[:, 2], c=dfkm['Cluster'], cmap='viridis', alpha=0.7)
ax.set_title('KMeans èšç±»ç»“æœ (PCA_3Dé™ç»´)')
ax.set_xlabel('ä¸»æˆåˆ† 1')
ax.set_ylabel('ä¸»æˆåˆ† 2')
ax.set_zlabel('ä¸»æˆåˆ† 3')
plt.colorbar(sc, label='Cluster')
plt.savefig(os.path.join(output_path, 'KMeans_PCA_3D.png'))
#plt.show()
plt.close()

''' t-SNE 2ç»´å¯è§†åŒ– '''
tsne = TSNE(n_components=2, random_state=42, perplexity=30, n_iter=1000)
df_tsne = tsne.fit_transform(dfkm.drop('Cluster', axis=1))

plt.figure(figsize=(8, 6))
plt.scatter(df_tsne[:, 0], df_tsne[:, 1], c=dfkm['Cluster'], cmap='viridis', alpha=0.7)
plt.title('KMeans èšç±»ç»“æœ (t-SNEé™ç»´)')
plt.xlabel('t-SNE ç»´åº¦ 1')
plt.ylabel('t-SNE ç»´åº¦ 2')
plt.colorbar(label='Cluster')
plt.savefig(os.path.join(output_path, 'KMeans_tSNE_2D.png'))
#plt.show()
plt.close()

''' èšç±»ä¸­å¿ƒçƒ­åŠ›å›¾  '''
centroids_df = pd.DataFrame(centroids, columns=dfkm.drop('Cluster', axis=1).columns)
centroids_df = centroids_df.T

plt.figure(figsize=(1 + user_K, max(6, centroids_df.shape[0]*0.5)))  # é«˜åº¦éšç‰¹å¾æ•°è‡ªåŠ¨è°ƒæ•´
sns.heatmap(centroids_df, annot=False, cmap='coolwarm', cbar=True)
plt.title('KMeans èšç±»ä¸­å¿ƒçƒ­åŠ›å›¾')
plt.xlabel('Cluster')
plt.ylabel('ç‰¹å¾')
plt.tight_layout()
plt.savefig(os.path.join(output_path, 'èšç±»ä¸­å¿ƒçƒ­åŠ›å›¾.png'))
#plt.show()
plt.close()


print(f'èšç±»åˆ†æå·²å®Œæˆï¼Œèšç±»ç»“æœåŠå¯è§†åŒ–å›¾å½¢å·²ä¿å­˜è‡³{output_path}')


''' ----------------------------------'''
''' ç»“æ„æ–¹ç¨‹ '''
# å®šä¹‰è·¯å¾„æ¨¡å‹
model_desc = """
# æµ‹é‡æ¨¡å‹
User =~ Age + Spending + Gender_Female + Identity_graduate + Identity_part + Identity_work
Emo =~ Loneliness + Depre 
Reason =~ ReasonLonely + ReasonEmo + ReasonSupport + ReasonSocial + ReasonEnter + ReasonNovel + ReasonOther 
Exper=~ ExperOverall + ExperNature + ExperSupport + ExperPrivacy + ExperPersonal
Impor =~ ImporNature + ImporSupport + ImporPrivacy + ImporPersonal 
Scenario =~ ScenarioSleep + ScenarioWork + ScenarioDown + ScenarioNovel + ScenarioTravel + ScenarioOther 
NotReason =~ NotReasonProduct + NotReasonNece + NotReasonPrivacy + NotReasonOther
Accept =~ Try + ConcernSocial + ConcernMoral + ConcernPrivacy + ConcernExper + ConcernNot + ConcernOther 
WTP =~ Pay + Price


# ç»“æ„æ¨¡å‹
UsedFreq ~ User + Emo
Impor ~ Reason + Exper + UsedFreq
Accept ~ Exper + Impor + NotReason
WTP ~ Accept
Market ~ UsedFreq + Exper + WTP + Accept + User
"""

# åˆ›å»ºSEMæ¨¡å‹
model = Model(model_desc)

# å°†æ•°æ®åŠ è½½åˆ°æ¨¡å‹ä¸­
model.fit(dfuf)

# è¾“å‡ºæ¨¡å‹ä¼°è®¡ç»“æœ
params = model.inspect()
print("æ¨¡å‹å‚æ•°ä¼°è®¡ç»“æœï¼š")
print(params)
params.to_excel(os.path.join(output_path,"SEMæ¨¡å‹å‚æ•°.xlsx"), index=False)

# è®¡ç®—æ‹Ÿåˆåº¦
fit = calc_stats(model) 
print("\næ‹Ÿåˆåº¦æŒ‡æ ‡ï¼š")
print(fit.T)
fit.to_excel(os.path.join(output_path,"SEMæ‹Ÿåˆåº¦.xlsx"), index=False)


''' --------------------------------------- '''
''' ç»“æ„æ–¹ç¨‹ä½œå›¾ '''

# å˜é‡å­—å…¸
varname_mapping = {
    'User': 'ç”¨æˆ·ä¿¡æ¯',
    'Age': 'å¹´é¾„',
    'Spending': 'æœˆæ¶ˆè´¹é¢',
    'Gender_Female': 'æ€§åˆ«ï¼ˆå¥³æ€§=1ï¼‰',
    'Identity_graduate': 'èº«ä»½ï¼ˆç ”ç©¶ç”Ÿï¼‰',
    'Identity_part': 'èº«ä»½ï¼ˆå…¼èŒï¼‰',
    'Identity_work': 'èº«ä»½ï¼ˆå·¥ä½œï¼‰',
    'Emo': 'æƒ…ç»ªçŠ¶æ€',
    'Loneliness': 'å­¤ç‹¬æ„Ÿ',
    'Depre': 'æŠ‘éƒåº¦',
    'Reason': 'ä½¿ç”¨AIçš„åŸå› ',
    'ReasonLonely': 'ç¼“è§£å­¤ç‹¬',
    'ReasonEmo': 'ç¼“è§£æƒ…ç»ª',
    'ReasonSupport': 'è·å–æ”¯æŒ',
    'ReasonSocial': 'ç¤¾äº¤äº’åŠ¨',
    'ReasonEnter': 'å¨±ä¹æ”¾æ¾',
    'ReasonNovel': 'æ–°å¥‡æ¢ç´¢',
    'ReasonOther': 'å…¶ä»–åŸå› ',
    'Exper': 'AIä½¿ç”¨ä½“éªŒ',
    'ExperOverall': 'æ€»ä½“ä½“éªŒ',
    'ExperNature': 'è‡ªç„¶äº¤äº’',
    'ExperSupport': 'æ”¯æŒæ€§',
    'ExperPrivacy': 'éšç§æ€§',
    'ExperPersonal': 'ä¸ªæ€§åŒ–',
    'Impor': 'AIåŠŸèƒ½åå¥½',
    'ImporNature': 'åå¥½-è‡ªç„¶äº¤äº’',
    'ImporSupport': 'åå¥½-æ”¯æŒæ€§',
    'ImporPrivacy': 'åå¥½-éšç§æ€§',
    'ImporPersonal': 'åå¥½-ä¸ªæ€§åŒ–',
    'Scenario': 'ä½¿ç”¨åœºæ™¯',
    'ScenarioSleep': 'åœºæ™¯-åŠ©çœ ',
    'ScenarioWork': 'åœºæ™¯-å·¥ä½œ',
    'ScenarioDown': 'åœºæ™¯-æƒ…ç»ªä½è½',
    'ScenarioNovel': 'åœºæ™¯-æ¢ç´¢',
    'ScenarioTravel': 'åœºæ™¯-å‡ºè¡Œ',
    'ScenarioOther': 'åœºæ™¯-å…¶ä»–',
    'NotReason': 'ä¸ä½¿ç”¨AIçš„åŸå› ',
    'NotReasonProduct': 'ä¸ä½¿ç”¨-åŠŸèƒ½ä¸è¶³',
    'NotReasonNece': 'ä¸ä½¿ç”¨-ä¸å¿…è¦',
    'NotReasonPrivacy': 'ä¸ä½¿ç”¨-éšç§æ‹…å¿§',
    'NotReasonOther': 'ä¸ä½¿ç”¨-å…¶ä»–',
    'Accept': 'AIæ¥å—åº¦',
    'Try': 'å°è¯•æ„æ„¿',
    'ConcernSocial': 'æ‹…å¿§-ç¤¾äº¤é£é™©',
    'ConcernMoral': 'æ‹…å¿§-ä¼¦ç†é£é™©',
    'ConcernPrivacy': 'æ‹…å¿§-éšç§é£é™©',
    'ConcernExper': 'æ‹…å¿§-ä½“éªŒé£é™©',
    'ConcernNot': 'æ‹…å¿§-ä¸ä½¿ç”¨ç†ç”±',
    'ConcernOther': 'æ‹…å¿§-å…¶ä»–',
    'WTP': 'AIæ”¯ä»˜æ„æ„¿',
    'Pay': 'ä»˜è´¹æ„æ„¿',
    'Price': 'ä»˜è´¹ä»·æ ¼',
    'Market': 'AIå¸‚åœºé¢„æœŸ',
}

# å›ºå®šæµ‹é‡æ¨¡å‹ç»“æ„
measurement_model = {
    'ç”¨æˆ·ä¿¡æ¯': ['å¹´é¾„', 'æœˆæ¶ˆè´¹é¢', 'æ€§åˆ«ï¼ˆå¥³æ€§=1ï¼‰', 'èº«ä»½ï¼ˆç ”ç©¶ç”Ÿï¼‰', 'èº«ä»½ï¼ˆå…¼èŒï¼‰', 'èº«ä»½ï¼ˆå·¥ä½œï¼‰'],
    'æƒ…ç»ªçŠ¶æ€': ['å­¤ç‹¬æ„Ÿ', 'æŠ‘éƒåº¦'],
    'AIæ¥å—åº¦': ['å°è¯•æ„æ„¿', 'æ‹…å¿§-ç¤¾äº¤é£é™©', 'æ‹…å¿§-ä¼¦ç†é£é™©', 'æ‹…å¿§-éšç§é£é™©', 'æ‹…å¿§-ä½“éªŒé£é™©', 'æ‹…å¿§-ä¸ä½¿ç”¨ç†ç”±', 'æ‹…å¿§-å…¶ä»–'],
    'AIæ”¯ä»˜æ„æ„¿': ['ä»˜è´¹æ„æ„¿', 'ä»˜è´¹ä»·æ ¼'],
}

# æå–ç»“æ„æ¨¡å‹è·¯å¾„å…³ç³»
params = params[params['p-value'] != '-'].copy()
params['p-value'] = pd.to_numeric(params['p-value'], errors='raise')

# ç­›é€‰ç»“æ„æ¨¡å‹è·¯å¾„å…³ç³» (op == '~')
structural_params = params[params['op'] == '~']

# æ„å»ºç»“æ„æ¨¡å‹å­—å…¸ {å› å˜é‡: {è‡ªå˜é‡: (Estimate, p)}}
structural_model = dict()
for _, row in structural_params.iterrows():
    dep = row['lval']
    indep = row['rval']
    estimate = row['Estimate']
    p_value = row['p-value']  # ç°åœ¨100% float

    dep_cn = varname_mapping.get(dep, dep)
    indep_cn = varname_mapping.get(indep, indep)

    if dep_cn not in structural_model:
        structural_model[dep_cn] = dict()
    structural_model[dep_cn][indep_cn] = (estimate, p_value)


# ä½œå›¾
dot = Digraph(comment='SEM Path Diagram')

# é…è‰²ï¼ˆæ›´æµ…ã€æ›´æŸ”å’Œï¼‰
latent_color = '#fbd5a1'        # æµ…æè‰²
observed_color = '#fff3b0'      # å¥¶æ²¹é»„
structural_edge_color = '#a0d9cb'  # æµ…è“ç»¿
measurement_edge_color = '#a3bfc8'  # æµ…ç°è“

# å…¨å±€å¸ƒå±€è°ƒæ•´
dot.attr(dpi='400')
dot.attr('graph', rankdir='LR', ranksep='1.5', nodesep='1.5', size='10,16!', ratio='compress')

# å­—ä½“å’ŒèŠ‚ç‚¹æ ·å¼
dot.attr('node', fontname='SimSun', fontsize='100', width='5.6', height='3.2', margin='0.4,0.3')
dot.attr('edge', fontname='SimSun', fontsize='80')

# æ·»åŠ æµ‹é‡æ¨¡å‹
for latent, observables in measurement_model.items():
    dot.node(latent, latent, style='filled', fillcolor=latent_color, shape='ellipse', fontcolor='black')
    for obs in observables:
        dot.node(obs, obs, style='filled', fillcolor=observed_color, shape='box', fontcolor='black')
        dot.edge(latent, obs, color=measurement_edge_color, penwidth='12')

# æ·»åŠ ç»“æ„æ¨¡å‹
for dependent, independents in structural_model.items():
    dot.node(dependent, dependent, style='filled', fillcolor=latent_color, shape='ellipse', fontcolor='black')
    for independent, (estimate, p_value) in independents.items():
        edge_style = 'solid' if p_value < 0.05 else 'dashed'
        label = f'{estimate:.2f}'
        dot.edge(independent, dependent, color=structural_edge_color, penwidth='12', style=edge_style, label=label, fontsize='80')

# è¾“å‡ºå›¾åƒ
dot.render(filename = 'Vertical_SEM_Path_Diagram', directory = output_path, format='png', cleanup=True)



    
    
    
    
    




    
    